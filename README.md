# Tarantool

## Обзор Tarantool

### Введение

**Tarantool** — это платформа для вычислений в памяти, разработанная **Mail.ru Group** и выпущенная в open-source в 2010 году. Изначально Tarantool был создан для снижения затрат на поддержание крупных кластеров MySQL в проекте **"Мой Мир"**. С тех пор Tarantool активно используется в других продуктах компании: Почта, Реклама, Облако и других.

### История создания Tarantool

- **2008 год**: Начало разработки.
- **2010 год**: Выпуск Tarantool в open-source.
- **Цель разработки**: Уменьшение затрат на содержание и обслуживание кластера MySQL, который хранил профили пользователей.

## Эволюция Tarantool

1. ### In-Memory Cache

   - **Описание**: Быстрый кеш, хранящий данные в оперативной памяти для ускорения доступа.
   - **Проблема**: При перезапуске системы данные терялись, что снижало эффективность.

2. ### In-Memory Persistent Cache

   - **Решение проблемы**: Добавлена возможность персистентности — сохранения снимков данных на диск.
   - **Преимущества**: Быстрое восстановление данных после падения благодаря загрузке из сохраненных снимков.

3. ### In-Memory Cache with Replication

   - **Добавлена репликация**: Возможность собирать кластеры, повышая производительность и отказоустойчивость.
   - **Преимущества**: Если один инстанс падает, другой продолжает работу, обеспечивая доступность данных.

4. ### In-Memory Key/Value DB

   - **Переход к модели key/value**: Упрощение работы с данными и расширение функциональности.

5. ### Multiindex DB

   - **Множественные индексы**: Возможность создавать несколько индексов по различным полям.
   - **Преимущества**: Ускорение поиска и выборки данных.

6. ### In-Memory Multiindex DB with Lua Functions

   - **Интеграция Lua**: Возможность писать хранимые процедуры и выполнять более гибкую обработку данных.

7. ### In-Memory Multiindex DB with Lua Cooperative Runtime

   - **Кооперативный runtime**: Добавление возможности выполнять несколько операций одновременно.
   - **Преимущества**: Разгрузка основного потока и повышение эффективности.

8. ### In-Memory Multiindex DB with Lua App Server

   - **Сервер приложений на Lua**: Возможность создания полноценных приложений внутри Tarantool.

9. ### Hybrid Storage Multiindex DB with SQL and Lua App Server

   - **Гибридное хранилище**: Мультииндексное хранилище с ограниченными функциями SQL и сервером приложений на Lua.

## Tarantool сегодня

Tarantool является платформой in-memory вычислений с гибкой схемой данных для эффективного создания высоконагруженных приложений.

### Версии Tarantool

- **Open Source**:

  - **Доступность**: Размещен на [GitHub](https://github.com/tarantool) и распространяется по лицензии Simplified BSD.
  - **Компоненты**: Ядро, коннекторы, модули, библиотеки и топологии.
  - **Сообщество**: Около 150 контрибьюторов.
  - **Продукты**: Cartridge, Kubernetes Operator, Queue.

- **Enterprise**:

  - **Дополнения**: Поддержка решений, enterprise-продукты (Tarantool Data Grid, Master Data Manager, Channel Control).
  - **Услуги**: Обучение, консалтинг, кастомная разработка.

## Архитектура Tarantool

### Основные особенности

- **In-Memory**: Все данные находятся в оперативной памяти, что обеспечивает низкие задержки доступа (latency).

- **Один поток доступа к данным**: Предотвращает проблемы конкурентного доступа.

- **Персистентность**:

  - **Write Ahead Log (WAL)**: Последовательная запись изменений на диск для сохранения данных при сбоях.
  - **Snapshot**: Периодическое сохранение состояния базы данных для ускорения восстановления.

- **Индексы**: Поддержка различных типов индексов для ускорения поиска.

- **Репликация WAL**: Обеспечивает актуальность данных на всех узлах кластера.

### Компоненты архитектуры

- **Основной поток (транзакционный поток)**:

  - Управляет **ареной** — областью памяти, где хранятся данные.
  - Данные организованы в **спейсы** (аналог таблиц) и **таплы** (аналог строк).

- **Индексы**:

  - Строятся из данных в арене.
  - Ускоряют поиск и выборку данных.

- **Событийный цикл**:

  - **Файберы**: Легковесные нити исполнения, выполняющие операции с данными.
  - **Кооперативная многозадачность**: Задачи выполняются последовательно, передавая управление.

- **Сетевой поток (IPROTO)**:

  - Принимает внешние запросы, парсит и обрабатывает соединения.
  - Вызывает соответствующие файберы для выполнения запросов.

- **Write Ahead Log (WAL) поток**:

  - Записывает изменения данных на диск в файлы с расширением `.xlog`.

- **Snap Daemon**:

  - Создает консистентные **snapshot'ы** базы данных для ускорения восстановления.

- **Репликация**:

  - **Relay поток**: Передает изменения из WAL другим инстансам.
  - **Applier файбер**: На стороне реплики принимает и применяет изменения.

### Процесс запуска Tarantool

1. **Загрузка Snapshot**: Восстановление состояния данных из последнего снимка.

2. **Применение WAL**: Воспроизведение изменений из файлов логов, произошедших после создания snapshot.

3. **Достройка индексов**: Построение вторичных индексов в памяти.

4. **Запуск LuaJIT приложения**: Инициализация пользовательского приложения на Lua.

## Интеграция Lua

### Почему Lua?

- **Простота**: Скриптовый язык, понятный инженерам и легко изучаемый.

- **Эффективность**: Высокоэффективный **JIT-компилятор (LuaJIT)** приближает производительность скриптов к коду на C.

- **Близость к данным**: Возможность выполнять логику непосредственно рядом с данными, избегая сетевых запросов.

### Кооперативный Runtime

- **Файберы**: Легковесные нити, реализующие кооперативную многозадачность.

- **Многозадачность**: Каждая задача выполняется полностью, пока не передаст управление следующей.

- **Преимущества**: Повышение производительности и эффективности использования ресурсов.

## Сервер приложений

- **Событийный цикл с файберами**: Управление выполнением задач и обработкой событий.

- **Неблокирующие библиотеки**: Работа с сетью без блокировок, возможность быть сервером и клиентом.

- **Библиотеки для работы с данными**: Упрощают взаимодействие с базой данных.

- **Встроенные функции**: Предоставляют инструменты для работы с БД и разработке приложений.

## Сравнение Tarantool с другими СУБД

### In-Memory платформы

**Похожесть**:

- In-memory DB с runtime для приложений.
- Гибкая кластеризация.
- Решение бизнес-задач.

**Отличия**:

- Уникальная архитектура.
- Собственный набор инструментов и коннекторов.
- Широкий спектр бизнес-сценариев.

### Реляционные базы данных

**Похожесть**:

- Данные организованы в таблицах (спейсах).
- Поддержка индексов.
- Возможность работы с SQL*.

**Отличия**:

- **In-Memory хранение**: Более высокая производительность.
- **Гибкая схема данных**: Возможность комбинировать схематичный и документный подходы.
- **SQL не основной инструмент**: Основной язык — Lua.

\*SQL доступен в ограниченном виде.

### Key-Value хранилища

**Похожесть**:

- Режим работы key-value.
- Замена для Memcached и Redis.

**Отличия**:

- Поддержка вторичных индексов.
- Транзакции и итераторы.
- Богатый функционал для работы с данными.

### Документоориентированные базы данных

**Похожесть**:

- Хранение документов в формате MessagePack.
- Поддержка Avro Schema для работы с документами.

**Отличия**:

- Ограниченная индексация по документам.

### Колоночные базы данных

- **Tarantool не является колоночной базой данных**, но может дополнять их.

- **Batching**: Использование Tarantool в качестве агрегирующего слоя для накопления данных и передачи их в колоночную БД.

## Кейсы использования Tarantool

### Плохие сценарии

- **Аналитические запросы (OLAP)**:

  - Причина: Tarantool использует один поток, что неэффективно для аналитики.

### Хорошие сценарии

- **Высокочастотные микротранзакции (OLTP)**.
- **Профили пользователей**.
- **Счетчики и признаки**.
- **Кеш-прокси к данным**.
- **Брокеры очередей**.

**Причины**:

- Гарантированная низкая задержка (latency).
- Быстрый доступ к данным.
- Возможность работы рядом с данными без сетевых задержек.

## Поддерживаемые платформы

- **Операционные системы**:

  - Linux (x86/64)
  - BSD
  - macOS

- **Контейнеризация**:

  - Docker
  - Kubernetes (с помощью Kubernetes Operator)

- **Облачные среды**:

  - Совместим с различными облачными платформами.

- **ARM и IoT**:

  - Поддержка ARM-архитектуры (например, Raspberry Pi).

## Функциональность Tarantool

### Примитивы в Lua

- **Типы данных**:

  - Числа (`number`)
  - Строки (`string`)
  - Таблицы (`table`)
  - Булевы значения (`boolean`)

- **Управляющие конструкции**:

  - Условия (`if`, `else`)
  - Циклы (`for`, `while`)
  - Функции (`function`)

### Типы данных в Tarantool

- `string`: Строковый тип.
- `number`: Числовой тип.
- `integer`: Целые числа.
- `double`: Числа с плавающей точкой.
- `scalar`: Скалярные значения.
- `decimal`: Десятичные числа.
- `boolean`: Логический тип.
- `map`: Карты (ассоциативные массивы).
- `array`: Массивы.
- `uuid`: Универсальные уникальные идентификаторы.
- `datetime`: Тип для хранения даты и времени.
- `any`: Универсальный тип, принимающий любые значения.

### Примитивы хранения данных

- **Тапл (Tuple)**:

  - Кортеж данных, аналог строки в таблице.

- **Спейс (Space)**:

  - Коллекция таплов, аналог таблицы в SQL.
  - Имеет определенный **движок хранения (Engine)**:
    - `memtx`: In-memory движок для высокой производительности.
    - `vinyl`: Дисковый движок для хранения больших объемов данных.

- **Индексы**:

  - **Первичный индекс (Primary Index)**: Обязателен для каждого спейса.
  - **Вторичные индексы**: Дополнительные индексы для ускорения поиска.
  - **Композитные индексы**: Индексы по нескольким полям.

- **Типы индексов**:

  - **`tree (B⁺*)`**: Основной тип индекса для упорядоченного доступа.
  - **`hash`**: Для быстрого доступа по ключу.
  - **`bitmap`**: Для булевых значений и битовых полей.
  - **`rtree`**: Для геопространственных данных.
  - **`functional`**: Индексы на основе функций.
  - **`json path`**: Индексы по JSON-путям внутри документов.



## Функциональность сервера приложений

Для обеспечения работы с сервером, он должен слушать определенный порт и общаться по определенному протоколу. В Tarantool по умолчанию используется бинарный протокол — IPROTO. Обычно сервер приложений запускают на порту 3301, но его можно настроить по своему усмотрению. IPROTO поддерживает настройку аутентификации и авторизации пользователей. При подключении к серверу можно получить доступ к консоли разработчика и выполнить необходимые команды.

## Файберы и каналы

### Работа с файберами

Файбер — это легковесная нить исполнения, работающая в одном процессе. Файберы используются для реализации кооперативной многозадачности. В один момент времени работает только один файбер. Файберы в кооперативной многозадачности отдают управление добровольно.

- **Просмотр запущенных файберов**: `fiber.info()`
- **Создание файбера**: `fiber.create(function_name)`
- **Проверка состояния файбера**: `fiber_object:status()`
- **Состояния файбера**:
  - `running` — запущен
  - `suspended` — ожидает исполнения
  - `dead` — завершен
- **Передача управления**:
  - `fiber.yield()` — явная передача управления
  - `fiber.sleep(T)` — неявная передача управления, происходит при вызове операций, ожидающих завершения

### Работа с каналами

Канал — это примитив синхронизации и инструмент межфайберного взаимодействия. Его главная задача — передача сообщения от файбера к файберу.

- **Создание канала**: `fiber.channel()`
- **Создание канала с буферной емкостью**: `fiber.channel(S)`, где `S` — буферная емкость канала
- **Основные методы**:
  - Отправка сообщения: `channel:put(message)`
  - Получение сообщения: `message = channel:get()`

Если операция `put` или `get` не может быть выполнена сразу, канал неявно заберет управление у этого файбера и отдаст какому-то другому.

## Работа по сети

Tarantool поддерживает работу с сетью. Помимо файберов и каналов, существует модуль `socket`, который позволяет обмениваться данными с локальными или удаленными хостами. 

Пример работы с HTTP-клиентом:

```lua
local http_client = require('http.client')
local response = http_client.get('http://mail.ru')
```

## Встроенные модули

- **`clock`** — работа со временем
- **`http`** — HTTP-клиент 
- **`uri`** — работа с URI
- **`csv`** — чтение и запись CSV-файлов
- **`fun`** — функциональное программирование
- **`buffer`** — работа с буферами и бинарными данными
- **`digest` и `crypto`** — шифрование и криптография
- **`socket`** — работа с сетью
- **`fio`** — неблокирующая работа с файловой системой
- **`json`, `yaml`, `msgpack`** — сериализация данных
- **`iconv`, `utf8`** — работа с кодировками
- **`uuid`** — генерация UUID

## Установка дополнительных модулей

**Luarocks** — пакетный менеджер для установки дополнительных Lua-модулей. Также существуют модули, написанные специально для Tarantool.

## Коннекторы

Коннекторы — это API, позволяющие использовать Tarantool с различными языками программирования. Существуют коннекторы для:

- C
- Go 
- Java
- Python

А также коннекторы, поддерживаемые сообществом.



# Типы репликации

Репликация — это механизм синхронизации содержимого нескольких копий объекта. Репликация бывает **синхронная** и **асинхронная**. Синхронная репликация дает нам надежность, асинхронная — скорость. Также репликация может быть **однонаправленной** и **двунаправленной**. В первом случае мы получаем консистентность, во втором — доступность. Далее подробнее о каждом типе.

## Синхронная репликация

Когда мы говорим о синхронной репликации, мы часто имеем в виду **кворумную запись** — транзакция подтверждается в кластере, когда она фиксируется еще где-нибудь (на реплике, нескольких репликах и т.д.). Синхронная репликация надежнее, чем асинхронная — если отключить машину после подтверждения транзакции, с высокой вероятностью транзакция не потеряется. При этом она медленнее, чем асинхронная, и у нее выше шанс отказа в обслуживании — если несколько реплик сломались, и рабочего количества не хватает для создания кворума, работа останавливается. Синхронная репликация сложная, поэтому в реальных системах встречается крайне редко.

## Асинхронная репликация

Более частый способ организации репликации — **асинхронный**. В этом случае транзакция подтверждается после записи хотя бы на одну ноду. На остальные ноды изменения распространяются асинхронно. Шанс потерять данные выше, чем у синхронной репликации.

## Однонаправленная репликация

По сути, это классическая репликация **Master-Slave (Leader-Follower)**. Есть одна нода-мастер, которая распространяет свои изменения на все остальные реплики (синхронно или асинхронно — неважно). Ее главная проблема — остановка работы в случае потери мастер-ноды. Потребуется некоторое время, чтобы сделать из slave-ноды нового мастера.

## Двунаправленная репликация

В народе называется **Master-Master** (или **Multi Master**). Мастера обмениваются изменениями в обе стороны, писать можно на любой мастер. Если вы потеряете одного мастера, есть еще один, который не даст системе упасть.

# Топологии репликации в Tarantool

На самом деле, в Tarantool можно реализовать любую топологию репликации:

- **Master-Replica** — ставим одного мастера, он передает изменения в реплику.
- **Master-Master** — поднимаем две ноды, заставляем их обмениваться данными в обе стороны.
- **Full-Mesh** — берем какое-то количество нод, выбираем мастера, заставляем все ноды обмениваться данными, даже реплики с репликами.

# Как репликация работает внутри Tarantool?

Слева изображен Мастер, справа — реплика. В процессе репликации участвуют 3 сущности:

1. **WAL-тред**. Его задача — записывать xlog'и в файлы журнала, который хранится на диске.
2. **RELAY-тред**. Его задача — читать xlog'и, отлавливать изменения, передавать их по сети на реплику.
3. **Файбер Applier**. Его задача — читать и парсить то, что отправляет мастер, и анализировать, нужно ли применять изменения.

Чтобы позволить разным нодам обмениваться изменениями, эти изменения нужно адресовать — для этого в Tarantool есть счетчик транзакций **LSN** (*Log Sequence Number*).

# Настройка репликации в Tarantool

## Ограничения репликации

### Идентифицированные реплики:

- количество ≤ 32
- идентифицированы
- могут генерировать изменения

### Анонимные реплики:

- `id = 0`
- `uuid` нет
- получают `snapshot`, `XLOG`'и
- ничего не передают в кластер, работают только на чтение
- количество не ограничено

## Опции `box.cfg` для настройки репликации

- `read_only` — показывает, можно ли производить на ноде изменения или нельзя.
- `replication` — показывает список серверов (мастеров), откуда нода будет скачивать изменения.
- `replication_connect_quorum` — показывает, какое количество реплик необходимо для принятия решения о состоянии кластера (в случаях проблем с сетью или падения нескольких нод).
- `replication_connect_timeout` — интервал, в течение которого репликация пытается подключиться к удаленным узлам (в случаях проблем с сетью).
- `replication_skip_conflict` — директива для пропуска конфликтующих транзакций, используется только для восстановления репликации.

# Master-Master репликация

## Нереплицируемые спейсы

В Tarantool существует два типа:

### Node local space (`is_local`)

- данные хранятся только на сервере
- пишет в `WAL`
- рестарт ноды ≠ потеря данных

### Temporary space

- данные хранятся только на сервере
- не пишет в `WAL`
- рестарт ноды = потеря данных
- быстрый

# Проблематика Master-Master репликации

- **Одновременное изменение** — в оба мастера единовременно могут поступать разные данные, в таком случае репликация не способна их синхронизировать.
- **Интервал меньше лага репликации**
- Обмен `xlog` корректен

# Механизмы разрешения конфликтов

- **Использование коммутативных операций** — вместо `=` использовать `+=`, чтобы после репликации данные на мастерах совпадали. Например, изначально `x` был равен `1`, один мастер получил транзакцию `x=2`, другой — `x=3`. Чтобы их уровнять, мы запишем их как `x+=2` и `x+=3`. После применения транзакций на одном мастере получим `x=3`, на другом `x=4`. В таком случае, после репликации мы придем к общему на двух мастерах значению `x=6`.
- **Временные метки у транзакций** — во время записи данных в мастер к транзакции будет дописываться `time=...`. Во время репликации каждый мастер оставит у себя ту транзакцию, которая была совершена позже. Вместо временных меток можно использовать другие параметры, которые позволят мастерам сравнить транзакции.
- **Триггер** — поставить триггер `before_replace` на спейсе при создании спейса. Используется при некоммутативных изменениях, например, вставках, и позволяет добиться консистентности данных на этапе репликации.

# Master-Slave репликация

Чаще всего используется топология **Full-Mesh**, когда существует один мастер и хотя бы две реплики, все ноды связаны между собой, через `box.cfg` управляется функция `read_only`. У такой топологии есть свои плюсы и минусы:

**Плюсы:**

- простота переключения

**Минусы:**

- много соединений
- распространение обновлений (большая амплификация на изменения)

# Мониторинг репликации

`box.info()` — самая важная функция для мониторинга. В первую очередь, в `box.info` со стороны реплики нужно смотреть на поле `upstream` — нормальная ситуация, когда у `upstream` стоит статус `follow`. Если стоит другой статус, значит, что-то пошло не так, и реплика не получает данные.

Бывают случаи, когда в поле `upstream` стоит статус `follow`, но реплика отстает по данным — репликация работает, но медленно. В таких случаях нужно смотреть на поле `lag` в `upstream`. Lag в норме должен быть меньше секунды, в идеале — равен `ping`.

Со стороны мастера нужно обращать внимание на поле `idle` в `upstream` — это время, прошедшее с последнего события. Все ноды обмениваются между собой heartbeat-сообщениями, которые сигнализируют об отсутствии проблем. Мастер отмечает, сколько времени прошло с последнего полученного heartbeat'а, и записывает эту информацию в `idle`. Периодичность отправки таких сообщений задается с помощью опции `replication_timeout`.


# Шардинг

Потребность в шардинге возникает в тот момент, когда появляется необходимость масштабировать данные. В случае Tarantool, например, когда заканчивается оперативная память.

Масштабирование бывает **вертикальным** и **горизонтальным**.

## Вертикальное

- То же количество серверов, но более мощных
- Ставим более мощные серверы
- Добавляем памяти
- Не бесконечно
- Самый крупный инстанс: 1Tb RAM @ 4Ghz CPU

## Горизонтальное

- Увеличение количества серверов, а не их мощности
- Ставим больше серверов
- Разделяем данные и нагрузку на части

Горизонтальное масштабирование дешевле и предпочтительнее, но для него необходимо разделить данные по разным серверам. Как это сделать? С помощью шардинга — вертикального или горизонтального.

### Вертикальное шардинг

- Длинная широкая таблица делится на длинные узкие таблицы

### Горизонтальное шардинг

- Длинная широкая таблица делится на короткие широкие таблицы

## Проблемы шардинга

- **Решардинг**
  - Время решардинга — сколько времени займёт разделение и распределение данных
  - Гранулярность — каким количеством данных проводить решардинг
  - Обслуживание (люди) — сколько человеческих ресурсов задействует решардинг
  - Избыточность — сколько ещё дополнительных данных нужно хранить, чтобы решардинг мог работать
  - Доступность — насколько система будет доступна во время решардинга
    - Чтение
    - Запись
  - Локальность — останутся ли связанные данные вместе или будут распределены

## Шардинг и доступность

- **Решардинг блокирует запись**
- Время блокировки зависит от гранулярности
- Отсутствие блокировок вредит консистентности (некоторые данные могут исчезнуть)
- Доступность зависит от латенси
- Латенси зависит от времени блокировки

## Виды горизонтального шардинга

Возвращаясь к проблеме разделения данных, горизонтальная опция используется чаще, но и она бывает разной. Технологий горизонтального шардинга множество, однако грубо мы можем разделить их на:

- **Классический шардинг**
- **Шардинг кольцом/диапазонами**
- **Управляемый шардинг**
- **Виртуальный шардинг**

### Классический шардинг

- Вычисление некоторой hash-функции от некоторого ключа и взятие остатка от деления на количество серверов.

**Проблемы такого подхода:**

- **Решардинг полный** — невозможно проводить шардирование без остановки работы
- **Время не квантовано** — нельзя добавить ещё один сервер, ожидая, что это повлияет на работу только пары других серверов; будут задействованы все серверы
- **Гранулярность**: нет
- **Обслуживание**: сложно
- **Локальность**: нет

### Шардинг диапазонами (кольцом)

- В качестве результатов функции шардинга выбираем не конкретный сервер, а некоторую точку из диапазона.

**Проблемы:**

- **Решардинг частичный** — из N серверов можно выключить один, чтобы решардинг прошёл успешно
- **Время не квантовано**
- **Гранулярность**: низкая
- **Обслуживание**: вручную
- **Доступность**: низкая
- **Локальность**: нет

### Шардинг адресацией (управляемый)

Встречается достаточно редко. Идея такая: мы зададим функцию, которая также от объекта будет возвращать шард/множество IP-адресов, только вместо математической функции это будет выполнено таблицей. Длина такой таблицы будет равна количеству объектов.

**Плюсы:**

- **Гранулярность** такой системы — построчная; значит, мы можем для каждого объекта решать, на каком шарде он должен находиться.
- **Решардинг** будет выполняться частями.
- **Доступность высокая** — можно отключить конкретный объект или множество, а не сервер.
- **Обслуживание** автоматическое.
- **Локальность** управляемая.

**Минусы:**

- **Оверхед** — каждая строка добавляет избыточность хранения данных.
- **Ограниченный ресурс** — если объектов будет много, мы столкнёмся с проблемами производительности таблицы или ограничением хранилища.
- **SPOF (Single Point of Failure)** — если с таблицей что-то случится, данные потеряются.
- **Таблицу шардинга придётся шардировать**.

### Виртуальный шардинг (VShard)

Этот вид шардинга используется в Tarantool. В чём идея: VShard добавляет виртуальные ноды в количестве, превышающем количество серверов. Такие ноды называются **бакеты**. Они привязываются к физическим шардам адресно, то есть по таблице. Объекты привязываются к бакету механизмом классического шардинга, то есть через hash-функцию.

**Плюсы такого подхода:**

- **Гранулярность**: бакет
- **Решардинг** частями
- **Доступность**: высокая — есть недоступность бакета в момент переноса, всё остальное будет доступно
- **Локальность** — чтобы связанные объекты лежали в одном месте, нужно положить их в один бакет
- **Обслуживание**: упрощено

## Работа VShard

**Как VShard переносит бакеты?**

Представим, что у нас есть некоторый бакет на сервере, к нему приходят клиентские запросы, которые читают и пишут из него. Мы добавляем новый сервер и хотим, чтобы этот бакет переехал на него. Когда последний пишущий запрос на бакете будет завершён, VShard блокирует бакет на запись и начинает его скачивание на новый сервер. **Чтение во время переноса будет доступно**. После того, как скачивание завершится, на старом сервере чтение и запись будут заблокированы, а на новом — станут доступны. Перенос завершён. Со временем процесс garbage collector почистит данные со старого сервера.

## VShard: Storage

У VShard есть две роли — **storage** и **router**.

- **Storage** — это группа серверов, репликасеты, которые хранят пользовательские данные. Каждый storage независим от другого.

## VShard: Router

- **Router** — stateless группа серверов, которая хранит и актуализирует карту bucket-storage, а также маршрутизирует запросы. Доступность данных зависит от того, как быстро router обновляет карту. Помимо основных задач, с помощью router выполняется первичная конфигурация (bootstrap).

## VShard: доступ к данным

**Вызов функции с указанием бакета**

- `callrw(bucket, 'func', ...)` — если нужно работать с мастером, читать или писать
- `callro(bucket, 'func', ...)` — если нужно работать с мастером, читать (если мастер недоступен — выполнится на реплике)
- `callbro(bucket, 'func', ...)` — механизм балансирующего чтения с мастера/реплики
- `callbre(bucket, 'func', ...)` — механизм балансирующего чтения с реплики

## Resharding и Rebalancer

Сбалансированность **resharding** и **rebalancing** определяется тем, где какое количество бакетов находится. VShard смотрит только на количество бакетов, а не на их содержимое/вес. Наша задача — равномерно разложить данные по бакетам. По умолчанию вес каждого шарда равен 1 — к примеру, у нас есть 12 тысяч бакетов и 3 шарда, они разделятся поровну, и на каждом шарде будет по 4 тысячи бакетов (1/3).

За процессом следит **Rebalancer** — отдельный storage, который с заданной периодичностью проверяет, одинаковое ли количество бакетов на всех шардах. Если у нас появился новый шард, Rebalancer просчитает стратегию, как оптимальнее всего заполнить его.

Иногда нам не нужно, чтобы Rebalancer перемещал данные. Для таких случаев существуют механизмы защиты от него:

- **Bucket referencing** — классический подход, позволяет "рефнуть" бакет на чтение/запись, подходит в качестве временного решения (рестарт ноды уберёт рефы с бакета). Рефнуть = создать ссылку на бакет, чтобы защитить его от ребалансировщика.
- **Bucket pinning** — то же самое, но подходит для постоянного решения.
- **Replicaset locking** — блокировка целого репликасета.

## Сколько бакетов нужно для продакшена? (на примерах)

### Пример расчёта для хранилища на 10 млрд объектов по 250 байт в среднем (объём 2.4 Тб)

- **Расчётный предел**: 10,000,000,000 объектов
- **Средний размер объекта**: ≈ 250 байт
- **Нормальный размер бакета**: мегабайты (5 Mb)
- **Кол-во объектов на бакет**: 5 Mb / 250 b ≈ 20,000
- **Кол-во бакетов**: 10,000,000,000 / 20,000 = **500,000**

### Пример расчёта для хранилища на 10 млн объектов по 50 Кб в среднем (объём: 500 Гб)

- **Расчётный предел**: 10,000,000 объектов
- **Средний размер объекта**: ≈ 50 Кб
- **Нормальный размер бакета**: мегабайты (5 Mb)
- **Кол-во объектов на бакет**: 5 Mb / 50 Kb ≈ 100
- **Кол-во бакетов**: 10,000,000 / 100 ≈ **100,000**

### Пример расчёта для хранилища на 100 млрд объектов по 16 байт в среднем (объём: 1.5 Тб)

- **Расчётный предел**: 100,000,000,000 объектов
- **Средний размер объекта**: ≈ 16 байт
- **Нормальный размер бакета**: мегабайты (5 Mb)
- **Кол-во объектов на бакет**: 5 Mb / 16 b ≈ 312,500
- **Кол-во бакетов**: 100,000,000,000 / 312,500 ≈ **320,000**

## Дополнительные материалы

- **Мастер-мастер репликация**
- **Как работает репликация в Tarantool**
- **VShard**
- **Ещё раз про репликацию**
- **Raft**
